# -*- coding: utf-8 -*-

import functools
from collections import namedtuple


from kep.parser.containers import get_blocks, get_year
from kep.parser.row_utils.utils import filter_value


def yield_datapoints(row_tuple: list, varname: str, year: int) -> iter:
    """Yield dictionaries containing individual datapoints based on *row_tuple* content.    

    :param row_tuple: tuple with annual value and lists of quarterly and monthly values
    :param varname: string like 'GDP_yoy'
    :param year: int
    :return: dictionaries ready to feed into pd.Dataframe
    """
    # a - annual value, just one number
    # qs - quarterly values, list of 4 elements
    # ms - monthly values, list of 12 elements
    a, qs, ms = row_tuple

    # annual value, yield if present
    if a:
        yield {'freq': 'a',
               'varname': varname,
               'year': year,
               'value': filter_value(a)}
    # quarterly values, yield if present
    if qs:
        for i, val in enumerate(qs):
            if val:
                yield {'freq': 'q',
                       'varname': varname,
                       'year': year,
                       'qtr': i + 1,
                       'value': filter_value(val)}
    # quarterly values, yield if present
    if ms:
        for j, val in enumerate(ms):
            if val:
                yield {'freq': 'm',
                       'varname': varname,
                       'year': year,
                       'month': j + 1,
                       'value': filter_value(val)}


def datablock_to_stream(label, datarows, splitter_func):
    if label:
        for row in datarows:
            a, qs, ms = splitter_func(row['data'])
            for dp in yield_datapoints(row_tuple=(a, qs, ms),
                                       year=get_year(row['head']),
                                       varname=label):
                yield dp


# TODO: need more information displayed about splitterfunc work
#       such as """WARNING: unexpected row with length 3"""

class Datapoints():
    """Emit a stream datapoints from *csv_dicts* according to *parse_def*."""

    def __init__(self, csv_dicts, parse_def):
        """
        csv_dicts: iterable of dictionaries with csv file content by row
                   each dictionary has 'head', 'data'
                   generated by CSV_Reader(path).yield_dicts()              
        parse_def: object containing header dict, units dict and splitter func name
                   generated by ParsingDefinition(path)
        """

        self.blocks = get_blocks(csv_dicts, parse_def)
        self.datapoints = list(self.walk_by_blocks())
        self.parse_def = parse_def

    def walk_by_blocks(self):
        for block in self.blocks:
            for datapoint in datablock_to_stream(label=block.label
                    , datarows=block.datarows
                    , splitter_func=block.splitter_func):
                yield datapoint

    def emit(self, freq):
        """Returns generator of dictionaries of datapoints as formatted 
           by yield_datapoints().

           param freq: 'a', 'q' or 'm' 
        """
        if freq in 'aqm':
            for p in self.datapoints:
                if p['freq'] == freq:
                    yield p
        else:
            raise ValueError(freq)

    def is_included(self, datapoint):
        """Return True if *datapoint* is in *self.datapoints*"""
        return datapoint in self.datapoints

    def unique_varnames(self):
        unique_varnames = []
        for freq in 'aqm':
            unique_varnames.extend([p['varname'] for p in self.emit(freq)])
        return sorted(list(set(unique_varnames)))

    @functools.lru_cache()
    def unique_varheads(self):
        vh = [x.split("__")[0] for x in self.unique_varnames()]
        return sorted(list(set(vh)))

    def not_imported(self):
        return [vh for vh in self.parse_def.unique_varheads() \
                             if vh not in self.unique_varheads()]

                
HASH_SEP = "^"


def key_hash(d):
    keys = ['freq', 'varname', 'year']
    if d['freq'] == 'm': keys.append('month')
    if d['freq'] == 'q': keys.append('qtr')
    return HASH_SEP.join([str(d[key]) for key in keys])

    
def unhash_varname(vn):
    return vn.split(HASH_SEP)[1]
            
  
hashed_point = namedtuple("Point", "key value")


class HashedValues():
    def __init__(self, datapoints):
        self.hash_value_tuples = [hashed_point(key_hash(d), d['value']) for d in datapoints]
        # [HashPoint(key='a^I__bln_rub^2013', value=13450.3), 
        #  HashPoint(key='a^I__bln_rub^2013', value=4378.4)]        
        self.dups = self.duplicates()

    def items(self, key):
        #dict-like acces to hashed values by key
        return [x for x in self.hash_value_tuples if x.key == key]

    def duplicates(self):
        hashes = [x.key for x in self.hash_value_tuples]
        seen_set = set()
        duplicate_set = set(x for x in hashes if x in seen_set or seen_set.add(x))
        return [self.items(k) for k in duplicate_set]

    def safe_duplicates(self):
        return [dp for dp in self.dups if dp[0].value == dp[1].value]

    def error_duplicates(self):
        return [dp for dp in self.dups if dp[0].value != dp[1].value]
    
    @staticmethod
    def __list_varnames__(dups_as_list_of_lists):
        _set = set(unhash_varname(_list[0].key) for _list in dups_as_list_of_lists)
        return list(_set)
    
    def safe_duplicates_varnames(self):
        return self.__list_varnames__(self.safe_duplicates())
        
    def error_duplicates_varnames(self):
        return self.__list_varnames__(self.error_duplicates())    
        
if __name__ == "__main__":
    # inputs
    import kep.reader.access as reader
    pdef = reader.get_pdef()
    csv_dicts = reader.get_csv_dicts()   
    
    # dataset
    d = Datapoints(csv_dicts, pdef)
    output = list(d.emit('a'))
    
    # FIXME: make unittest ----------------
    #assert len(d.duplicates()) == 0
    #extrapoint = {'freq': 'a', 'varname': 'GDP_bln_rub', 'year': 1999, 'value': 0}         
    #d.datapoints.append(extrapoint)
    #assert len(d.duplicates()) == 1
    #assert d.duplicates() == [extrapoint]
    
    #h = HashedValues(d.emit('a'))
    #print(h.error_duplicates())
    #assert h.error_duplicates() == []
    
    # end todo --------------------------
        
    print("\nParsing result on variable level")
    print  ("================================")    
    
    print("1. Variables included in parsing definitions, but not imported (not critical)"
          "\n   Possible reason: outdated parsing definition"
          "\n   Severity: HIGH")
    msg = ", ".join(d.not_imported())      
    print("   Variables:", msg) 
    print()
    
    
    print("\nWait while finding duplicates...")
    h = HashedValues(d.datapoints)
    
    print("\n2. Safe duplicates (same values for same date)" 
          "\n   Possible reason: table appears in CSV file twice"
          "\n   Severity: LOW")
    
    msg = ", ".join(h.safe_duplicates_varnames())
    print("   Variables:", msg)
    print()
          
    def echo(x):
        print("    {}:".format(x[0].key), ", ".join([str(z.value) for z in x]))

    for i, x in enumerate(h.safe_duplicates()):
        if i < 5:
            echo(x)
    print("    First 5 shown, total:", i)         

    
    print("\n3. Error duplicates (have different values for same date)"
          "\n   Possible reason: wrong header handling in algorithm or specfile"
          "\n   Severity: CRITICAL")
          
    msg = ", ".join(h.error_duplicates_varnames())
    print("   Variables:", msg)
    print()
    for i, x in enumerate(h.error_duplicates()):
        if i < 5:
            echo(x)
    print("    First 5 shown, total:", i)