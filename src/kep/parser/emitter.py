# -*- coding: utf-8 -*-

import itertools
from kep.parser.containers import get_blocks, get_year
from kep.parser.row_utils.utils import filter_value

def yield_datapoints(row_tuple: list, varname: str, year: int) -> iter:
    """Yield dictionaries containing individual datapoints based on *row_tuple* content.    
    
    :param row_tuple: tuple with annual value and lists of quarterly and monthly values
    :param varname: string like 'GDP_yoy'
    :param year: int
    :return: dictionaries ready to feed into pd.Dataframe
    """
    # a - annual value, just one number
    # qs - quarterly values, list of 4 elements
    # ms - monthly values, list of 12 elements
    a, qs, ms = row_tuple

    # annual value, yield if present
    if a:
        yield {'freq': 'a',
               'varname': varname,
               'year': year,
               'value': filter_value(a)}
    # quarterly values, yield if present
    if qs:
        for i, val in enumerate(qs):
            if val:
                yield {'freq': 'q',
                       'varname': varname,
                       'year': year,
                       'qtr': i + 1,
                       'value': filter_value(val)}
    # quarterly values, yield if present
    if ms:
        for j, val in enumerate(ms):
            if val:
                yield {'freq': 'm',
                       'varname': varname,
                       'year': year,
                       'month': j + 1,
                       'value': filter_value(val)}

               
def datablock_to_stream(label, datarows, splitter_func):
    if label:        
        for row in datarows:
            a, qs, ms = splitter_func(row['data']) 
            for dp in yield_datapoints(row_tuple=(a, qs, ms),
                               year=get_year(row['head']),
                               varname=label):
                yield dp

        
# TODO: need more information displayed about splitterfunc work
#       sauch as """WARNING: unexpected row with length 3"""

class Datapoints():
    """Emit a stream datapoints from *csv_dicts* according to *parse_def*."""

    def __init__(self, csv_dicts, parse_def):
        """
        csv_dicts: iterable of dictionaries with csv file content by row
                   each dictionary has 'head', 'data'
                   generated by CSV_Reader(path).yield_dicts()              
        parse_def: object containing header dict, units dict and splitter func name
                   generated by ParsingDefinition(path)
        """
        
        self.blocks = get_blocks(csv_dicts, parse_def)
        self.datapoints = list(self.walk_by_blocks())

    def walk_by_blocks(self):
        for block in self.blocks:
            for datapoint in datablock_to_stream(label=block.label
                                               , datarows=block.datarows
                                               , splitter_func=block.splitter_func):
                yield datapoint                
      
    def emit(self, freq):
        """Returns generator of dictionaries of datapoints as formatted 
           by yield_datapoints().
           
           param freq: 'a', 'q' or 'm' 
        """
        if freq in 'aqm':
            for p in self.datapoints:
                if p['freq'] == freq:
                    # Note: (1) 'freq' key will be redundant for later use in
                    #           dataframe, drop it
                    #       (2) without copy() pop() changes self.datapoints
                    # z = p.copy()
                    # z.pop('freq')
                    yield p
        else:
            raise ValueError(freq)

    def is_included(self, datapoint):
        """Return True if *datapoint* is in *self.datapoints*"""
        return datapoint in self.datapoints
        
    def unique_varnames(self):
        unique_varnames = []
        for freq in 'aqm':        
            unique_varnames.extend([p['varname'] for p in self.emit(freq)])
        return sorted(list(set(unique_varnames)))
    
    def unique_varheads(self):
        vh = [x.split("__")[0] for x in self.unique_varnames()]
        return sorted(list(set(vh)))
    
    def not_imported(self):
        return [vh for vh in parse_def.unique_varheads() \
                    if vh not in d.unique_varheads()]  

    
def key_hash(d):
    keys = ['freq','varname','year']
    if d['freq'] == 'm': keys.append('month')
    if d['freq'] == 'q': keys.append('qtr')
    return "^".join([str(d[key]) for key in keys])

        
class HashedValues():

    def __init__(self, datapoints):
        self.hash_value_tuples = [(key_hash(d), d['value']) for d in datapoints]
        hashes = [x[0] for x in self.hash_value_tuples]
        d = {y:hashes.count(y) for y in hashes}
        self.count = {k:v for k,v in d.items() if v>1}
        self.dups = self.duplicates()        
        
    def items(self, key):
        return [x for x in self.hash_value_tuples if x[0]==key]

    def duplicates(self):
        return [self.items(k) for k in self.count.keys()]
    
    def safe_duplicates(self):
        return [x for x in self.dups if x[0][1] == x[1][1]]
    
    def error_duplicates(self):
        return [x for x in self.dups if x[0][1] != x[1][1]]
        
         

if __name__ == "__main__":
    # inputs
    from kep.release import get_csv_dicts, get_pdef
    #FIXME - point to actual data using good year, month combination or calling mock dataset
    year = month = 0
    csv_dicts = get_csv_dicts(year, month)
    parse_def = get_pdef()

    # dataset
    d = Datapoints(csv_dicts, parse_def)
    output = list(d.emit('a'))
    
    # TODO: make unittest ----------------
    #assert len(d.duplicates()) == 0
    #extrapoint = {'freq': 'a', 'varname': 'GDP_bln_rub', 'year': 1999, 'value': 0}         
    #d.datapoints.append(extrapoint)
    #assert len(d.duplicates()) == 1
    #assert d.duplicates() == [extrapoint]
    
    #h = HashedValues(d.emit('a'))
    #print(h.error_duplicates())
    #assert h.error_duplicates() == []
    
    # end todo --------------------------
        
    print("\nParsing result on variable level")
    print("================================")    
    
    print("1. Variables included in parsing definitions, but not imported (not critical)"
          "\n   Possible reason: outdated parsing definition"
          "\n   Severity: LOW")
    print(", ".join(d.not_imported())) 
    
    print("\nWait while finding diplicates...")
    h = HashedValues(d.datapoints)
    
    print("\n2. Safe duplicates (same values for same date)" 
          "\n   Possible reason: table appears in CSV file twice"
          "\n   Severity: LOW")
    for i, x in enumerate(h.safe_duplicates()):
        if i < 15:
            print("    ", x)
    print("    First 15 shown, total:", i)                
    print("\n3. Error duplicates (have different values for same date)"
          "\n   Possible reason: wrong header handling in algorithm or specfile"
          "\n   Severity: CRITICAL")
    
    for i, x in enumerate(h.error_duplicates()):
        if i < 15:
            print("    ", x)
    print("    First 15 shown, total:", i)                
    
    
    
    

    
        
